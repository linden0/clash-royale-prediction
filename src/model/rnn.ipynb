{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import random\n",
    "import wave\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wavs_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all WAV files in a folder, converts them to numpy arrays, and plots their waveforms.\n",
    "    \"\"\"\n",
    "    # Iterate through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.wav'):  # Process only .wav files\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                # Open the WAV file\n",
    "                wf = wave.open(file_path, 'rb')  \n",
    "\n",
    "                # Get audio parameters\n",
    "                nframes = wf.getnframes()\n",
    "                framerate = wf.getframerate()\n",
    "\n",
    "                # Read the audio data\n",
    "                audio_data = wf.readframes(nframes)\n",
    "                audio_array = np.frombuffer(audio_data, dtype=np.int16)\n",
    "                \n",
    "                # Normalize the data\n",
    "                audio_array = audio_array / (2 ** (wf.getsampwidth() * 8 - 1))\n",
    "\n",
    "                # Time axis for plotting\n",
    "                time_axis = np.arange(0, len(audio_array)) / framerate\n",
    "\n",
    "                # Plot the waveform\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                plt.plot(time_axis, audio_array)\n",
    "                plt.xlabel(\"Time (seconds)\")\n",
    "                plt.ylabel(\"Amplitude\")\n",
    "                plt.title(f\"Waveform: {filename}\")\n",
    "                plt.grid()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Provide the folder path containing the .wav files\n",
    "# folder_path = '../data/archers/'\n",
    "# plot_wavs_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(data_root):\n",
    "    \"\"\"\n",
    "    Scans `data_root` directory. Each subfolder is a class label.\n",
    "    Returns:\n",
    "      - all_files: list of (wav_path, label_index) pairs\n",
    "      - classes: list of class names (strings) in sorted order\n",
    "    \"\"\"\n",
    "    classes = sorted(os.listdir(data_root))\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    \n",
    "    all_files = []\n",
    "    for cls_name in classes:\n",
    "        cls_folder = os.path.join(data_root, cls_name)\n",
    "        # Gather all WAV files in this folder\n",
    "        wav_files = glob.glob(os.path.join(cls_folder, \"*.wav\"))\n",
    "        for wav_file in wav_files:\n",
    "            all_files.append((wav_file, class_to_idx[cls_name]))\n",
    "    \n",
    "    return all_files, classes\n",
    "\n",
    "\n",
    "data_root = \"../../data\"  # Path to your data folder\n",
    "all_files, classes = gather_data(data_root)\n",
    "num_classes = len(classes)\n",
    "print(\"Found classes:\", classes)\n",
    "print(\"Total examples:\", len(all_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_files)  # Shuffle in-place\n",
    "\n",
    "split_idx = int(0.8 * len(all_files))  # 80% for train\n",
    "train_files = all_files[:split_idx]\n",
    "val_files   = all_files[split_idx:]\n",
    "\n",
    "print(f\"Train size: {len(train_files)}\")\n",
    "print(f\"Val   size: {len(val_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, file_list, n_mfcc=12, sr=16000, augment=False):\n",
    "        \"\"\"\n",
    "        file_list: list of (wav_path, label_index)\n",
    "        n_mfcc: number of MFCC coefficients\n",
    "        sr: sample rate to which audio is (optionally) resampled\n",
    "        augment: if True, apply random augmentations to training data\n",
    "        \"\"\"\n",
    "        self.file_list = file_list\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.sr = sr\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path, label = self.file_list[idx]\n",
    "        \n",
    "        # Load audio\n",
    "        waveform, sr = librosa.load(wav_path, sr=self.sr)\n",
    "\n",
    "        # Trim silence\n",
    "        if label != classes.index('_silence'):\n",
    "            waveform, _ = librosa.effects.trim(waveform, top_db=20)\n",
    "\n",
    "\n",
    "        # If augment is enabled, apply random transformations\n",
    "        if self.augment and len(waveform) > 2048:\n",
    "            # Random time-stretch (speed up/down by up to +/-10%)\n",
    "            if random.random() < 0.5:\n",
    "                rate = 1.0 + np.random.uniform(-0.1, 0.1)  # e.g., 0.9 to 1.1\n",
    "                waveform = librosa.effects.time_stretch(waveform, rate=rate)\n",
    "\n",
    "            # Random pitch shift (up/down by up to +/-2 semitones)\n",
    "            if random.random() < 0.5:\n",
    "                \n",
    "                n_steps = np.random.uniform(-2, 2)\n",
    "                waveform = librosa.effects.pitch_shift(waveform, sr=sr, n_steps=n_steps)\n",
    "\n",
    "            # Random time shift\n",
    "            # For example, shift by up to 10% of the wave length\n",
    "            if random.random() < 0.5:\n",
    "                shift_max = int(0.1 * len(waveform))\n",
    "                shift = np.random.randint(-shift_max, shift_max)\n",
    "                waveform = np.roll(waveform, shift)\n",
    "\n",
    "            # Random background noise injection\n",
    "            if random.random() < 0.5:\n",
    "                noise_level = np.random.uniform(0.01, 0.02)  # Adjust range as needed\n",
    "                noise = np.random.randn(len(waveform)) * noise_level\n",
    "                waveform = waveform + noise\n",
    "\n",
    "        # Now compute MFCC\n",
    "        mfcc = librosa.feature.mfcc(y=waveform, sr=sr, n_mfcc=self.n_mfcc, n_fft=1024)\n",
    "        mfcc = mfcc.T  # shape: (time_frames, n_mfcc)\n",
    "\n",
    "        # Convert to tensors\n",
    "        mfcc_tensor = torch.tensor(mfcc, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return mfcc_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (mfcc_tensor, label_tensor) pairs\n",
    "    We pad MFCC tensors on the time dimension (dim=0) so they have the same length.\n",
    "    \"\"\"\n",
    "    mfccs = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "\n",
    "    # Find max sequence length in this batch\n",
    "    max_len = max(m.shape[0] for m in mfccs)\n",
    "    n_mfcc  = mfccs[0].shape[1]  # number of MFCC coefficients\n",
    "\n",
    "    padded_mfccs = []\n",
    "    for m in mfccs:\n",
    "        length = m.shape[0]\n",
    "        pad_length = max_len - length\n",
    "        if pad_length > 0:\n",
    "            pad = torch.zeros(pad_length, n_mfcc)\n",
    "            m = torch.cat([m, pad], dim=0)\n",
    "        padded_mfccs.append(m)\n",
    "\n",
    "    # Stack along batch dimension\n",
    "    padded_mfccs = torch.stack(padded_mfccs, dim=0)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return padded_mfccs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8  # Small batch size, given few examples per class\n",
    "\n",
    "train_dataset = SpeechCommandsDataset(train_files, n_mfcc=12, sr=16000, augment=True)\n",
    "val_dataset   = SpeechCommandsDataset(val_files,   n_mfcc=12, sr=16000, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "val_loader   = DataLoader(val_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False,\n",
    "                          collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM expects (batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, input_size)\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "        # LSTM output: (batch_size, seq_length, hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Take the last time-step\n",
    "        out = out[:, -1, :]  # shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Fully-connected layer\n",
    "        out = self.fc(out)   # shape: (batch_size, num_classes)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "input_size = 12      # Because we used n_mfcc=12\n",
    "hidden_size = 48     # Hyperparameter - tune as needed\n",
    "num_layers = 2       # Hyperparameter - tune as needed\n",
    "learning_rate = 1e-3\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for mfccs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        mfccs, labels = mfccs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(mfccs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * mfccs.size(0)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mfccs, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            mfccs, labels = mfccs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(mfccs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * mfccs.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_loss = running_loss / total\n",
    "    val_acc  = correct / total\n",
    "    return val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "best_loss = float(\"inf\")\n",
    "best_model_weights = None\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "    \n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_weights = model.state_dict()\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}  |  Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}  |  Val   Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model_weights, \"lstm_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, wav_path, classes, device='cpu', sr=16000, n_mfcc=12):\n",
    "    \"\"\"\n",
    "    Predict the class for a single .wav audio file.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained PyTorch model.\n",
    "        wav_path (str): Path to the .wav file.\n",
    "        classes (list): List of class names (strings), where index=label.\n",
    "        device (str): 'cpu' or 'cuda' device.\n",
    "        sr (int): Sample rate to which the audio is (optionally) resampled.\n",
    "        n_mfcc (int): Number of MFCC features to compute.\n",
    "\n",
    "    Returns:\n",
    "        predicted_label (str): The predicted class name.\n",
    "        confidence (float): Softmax confidence for the predicted class.\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(\"/home/linden/Desktop/projects/cr_prediction/src/model/lstm_model.pt\", map_location=device))\n",
    "\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess audio\n",
    "    waveform, sr = librosa.load(wav_path, sr=sr)       # waveform: float32 numpy array\n",
    "    mfcc = librosa.feature.mfcc(y=waveform, sr=sr, n_mfcc=n_mfcc)  # shape: (n_mfcc, time_frames)\n",
    "    mfcc = mfcc.T  # shape: (time_frames, n_mfcc)\n",
    "    \n",
    "    # Convert to Torch tensor, add batch dimension => (1, time_frames, n_mfcc)\n",
    "    mfcc_tensor = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(mfcc_tensor)  # shape: (1, num_classes)\n",
    "        \n",
    "        # Optionally compute softmax to get probabilities\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Get predicted class index and its confidence\n",
    "        predicted_idx = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0, predicted_idx].item()\n",
    "\n",
    "    # Map index back to class label\n",
    "    predicted_label = classes[predicted_idx]\n",
    "    \n",
    "    return predicted_label, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model=model, wav_path='/home/linden/Desktop/projects/cr_prediction/11.wav', classes=classes, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
